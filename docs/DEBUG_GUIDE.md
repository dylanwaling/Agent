# ğŸ”§ Debug and Testing Guide

Comprehensive debugging and testing utilities for the Enhanced Document Q&A Pipeline.

## ğŸ“ Current Debug Tools

### `backend_debug.py` - Comprehensive Testing Suite
- **Purpose**: Complete system validation and performance testing
- **Features**:
  - System component testing (imports, Docling, Ollama, embeddings)
  - Database content inspection (FAISS index validation)
  - Pipeline initialization and document processing tests
  - Enhanced search functionality validation
  - Q&A functionality testing with real queries
  - Edge case testing (empty queries, non-existent docs, emojis)
  - Performance benchmarking
- **Usage**: `python backend_debug.py`
- **Expected Results**: All tests should pass, ~6ms search time

### `backend_logic.py` - Core Pipeline with Debug Methods
- **Purpose**: Main DocumentPipeline class with built-in debugging
- **Debug Methods**:
  - `debug_search(query)` - Returns detailed search results with scores
  - `search(query, score_threshold)` - Enhanced search with configurable thresholds
  - `ask(question)` - Q&A with source attribution
- **Usage**: Import and use directly for custom debugging

## ğŸ§ª Testing Workflow

### 1. System Health Check
```bash
python backend_debug.py
```

**What to look for:**
- âœ… All 7 system components pass
- âœ… FAISS index exists and loads
- âœ… Documents directory has files
- âœ… Ollama responds with llama3:latest

### 2. Search Functionality Validation
The debug script tests these queries:
- `"Invoice_Outline_-_Sheet1_1.pdf"` â†’ Should find Invoice PDF as #1
- `"product manual"` â†’ Should find product_manual.md as #1
- `"company handbook"` â†’ Should find company_handbook.md as #1
- `"algebra operations"` â†’ Should find Math PDF as #1

### 3. Performance Benchmarks
Expected metrics:
## ğŸš€ Quick Debugging Commands

### Manual Testing Examples
```python
from backend_logic import DocumentPipeline

# Initialize and test
pipeline = DocumentPipeline()
pipeline.load_index()

# Test search with debug info
debug_info = pipeline.debug_search("invoice outline")
print(f"Query: {debug_info['query']}")
print(f"Results: {debug_info['total_results']}")
for r in debug_info["results"][:3]:
    print(f"{r['rank']}. {r['source']} (score: {r['score']:.3f})")

# Test Q&A
result = pipeline.ask("What is in the invoice document?")
print(f"Answer: {result['answer'][:100]}...")
print(f"Sources: {[s['source'] for s in result['sources']]}")
```

### Performance Testing
```python
import time
from backend_logic import DocumentPipeline

pipeline = DocumentPipeline()
pipeline.load_index()

# Test search speed
queries = ["invoice", "manual", "handbook", "math"]
times = []

for query in queries:
    start = time.time()
    results = pipeline.search(query)
    end = time.time()
    times.append(end - start)
    print(f"{query}: {len(results)} results in {(end-start)*1000:.1f}ms")

print(f"Average: {sum(times)/len(times)*1000:.1f}ms")
```

## ï¿½ Troubleshooting Common Issues

### Issue: No search results
```python
# Check if index exists
from pathlib import Path
index_path = Path("data/index/faiss_index.faiss")
print(f"Index exists: {index_path.exists()}")

# Check document count
pipeline = DocumentPipeline()
if pipeline.load_index():
    test_results = pipeline.search("test", score_threshold=10.0)  # Very lenient
    print(f"Total indexed chunks: {len(test_results)}")
```

### Issue: Ollama connection problems
```python
from langchain_ollama import OllamaLLM

try:
    llm = OllamaLLM(model="llama3:latest")
    response = llm.invoke("Hello")
    print(f"Ollama working: {response[:50]}...")
except Exception as e:
    print(f"Ollama error: {e}")
    print("Run: ollama serve")
```

### Issue: Poor search results
```python
# Check score thresholds
pipeline = DocumentPipeline()
pipeline.load_index()

query = "your query here"
results = pipeline.search(query, score_threshold=5.0)  # Very lenient

print(f"Found {len(results)} results:")
for i, r in enumerate(results[:5]):
    print(f"{i+1}. {r['source']} (score: {r['relevance_score']:.3f})")
```

### Run Full Pipeline Test:
```bash
# Run all debug files in sequence (Windows PowerShell)
python debug-1-extraction.py; python debug-2-chunking.py; python debug-3-embedding.py; python debug-4-search.py; python debug-5-chat.py
```

## ğŸ” What Each Debug File Tests

### Step 1: Extraction Debug
- âœ… Internet connectivity
- âœ… Docling installation
- âœ… AI model caching
- âœ… PDF processing (ArXiv paper)
- âœ… HTML processing (GitHub page)
- âœ… Sitemap extraction
- âœ… Performance timing
- âœ… Content validation

### Step 2: Chunking Debug
- âœ… Tokenizer functionality
- âœ… Document extraction
- âœ… Chunker configuration
- âœ… Chunk generation
- âœ… Content quality analysis
- âœ… Metadata preservation
- âœ… Performance metrics

### Step 3: Embedding Debug
- âœ… SentenceTransformer model
- âœ… LanceDB operations
- âœ… Embedding generation
- âœ… Metadata processing
- âœ… Database insertion
- âœ… Memory usage
- âœ… Data consistency

### Step 4: Search Debug
- âœ… Database connectivity
- âœ… Model consistency
- âœ… Content analysis
- âœ… Search accuracy
- âœ… Performance timing
- âœ… Result quality
- âœ… Query variations

### Step 5: Chat Debug
- âœ… Ollama service status
- âœ… Model availability
- âœ… API functionality
- âœ… Context retrieval
- âœ… Response generation
- âœ… Streaming responses
- âœ… Error handling

## ğŸ”§ Troubleshooting Guide

### Common Issues & Solutions:

**âŒ Internet Connection Failed**
- Check network connectivity
- Verify firewall settings
- Try different URLs

**âŒ Ollama Connection Failed**
- Start Ollama service: `ollama serve`
- Check if running: `ollama list`
- Verify port 11434 is available

**âŒ Model Not Found**
- Install models: `ollama pull llama3`
- Check available models: `ollama list`

**âŒ Database Not Found**
- Run step 3 first: `python 3-embedding.py`
- Check data directory exists

**âŒ Poor Search Results**
- Verify embeddings: `python 3-embedding-debug.py`
- Check database content: `python debug_utils.py`

**âŒ Slow Performance**
- Check model caching: `python 1-extraction-debug.py`
- Monitor memory usage: `python 3-embedding-debug.py`

## ğŸ“Š Expected Output

Each debug script will show:
- âœ… **Green checkmarks**: Tests passed
- âŒ **Red X marks**: Tests failed
- âš ï¸ **Yellow warnings**: Potential issues
- ğŸ“Š **Statistics**: Performance metrics
- ğŸ’¡ **Tips**: Improvement suggestions

## ğŸ¯ Best Practices

1. **Run in sequence**: Follow steps 1â†’2â†’3â†’4â†’5
2. **Check prerequisites**: Ensure Ollama is running before step 5
3. **Monitor performance**: Watch timing and memory usage
4. **Verify data**: Check database content before testing search
5. **Test variations**: Try different queries and models

## ğŸ”„ Re-running Tests

- **Safe to re-run**: All debug scripts are non-destructive
- **Database recreation**: Step 3 debug may recreate test tables
- **Cache usage**: Subsequent runs will be faster due to caching

These debug utilities provide comprehensive testing and troubleshooting capabilities for the entire pipeline. Use them to identify issues, verify functionality, and optimize performance at each step.
